# Webown - Central Scraping Application

Application Python centrale pour le scraping de sites web de recherche de logement.

## ğŸ—ï¸ Architecture

L'application est construite avec :
- **Python 3.11** - Langage principal
- **PostgreSQL** - Base de donnÃ©es pour stocker les annonces
- **Redis** - Cache et gestion des queues (prÃ©vu pour futures fonctionnalitÃ©s)
- **Docker Compose** - Orchestration des services
- **APScheduler** - Planification des tÃ¢ches de scraping
- **BeautifulSoup4** - Parsing HTML
- **SQLAlchemy** - ORM pour la base de donnÃ©es

## ğŸ“‹ FonctionnalitÃ©s

- âœ… Scraping modulaire pour plusieurs sources :
  - Leboncoin.fr
  - SeLoger.com
  - La Carte des Coloc
- âœ… Scheduler automatique pour rÃ©cupÃ©rer les derniÃ¨res annonces
- âœ… Base de donnÃ©es PostgreSQL pour la persistance
- âœ… SystÃ¨me de logging complet
- âœ… Gestion des doublons (basÃ©e sur source + source_id)
- âœ… Configuration via variables d'environnement

## ğŸš€ Installation et DÃ©marrage

### PrÃ©requis

- Docker et Docker Compose installÃ©s
- Git (optionnel)

### DÃ©marrage rapide

1. **Cloner le projet** (si nÃ©cessaire)
```bash
git clone <repository-url>
cd webown
```

2. **Configurer les variables d'environnement**
```bash
cp .env.example .env
# Ã‰diter .env selon vos besoins
```

3. **DÃ©marrer l'application**
```bash
docker-compose up -d
```

4. **Voir les logs**
```bash
docker-compose logs -f app
```

### ArrÃªter l'application

```bash
docker-compose down
```

Pour supprimer aussi les volumes (base de donnÃ©es) :
```bash
docker-compose down -v
```

## ğŸ“ Structure du Projet

```
webown/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py              # Configuration de l'application
â”‚   â”œâ”€â”€ database.py            # Connexion Ã  la base de donnÃ©es
â”‚   â”œâ”€â”€ models.py              # ModÃ¨les de donnÃ©es
â”‚   â”œâ”€â”€ scheduler.py           # Gestionnaire de scheduler
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py            # Classe de base pour les scrapers
â”‚   â”‚   â”œâ”€â”€ leboncoin.py       # Scraper Leboncoin
â”‚   â”‚   â”œâ”€â”€ seloger.py         # Scraper SeLoger
â”‚   â”‚   â”œâ”€â”€ carte_coloc.py     # Scraper La Carte des Coloc
â”‚   â”‚   â””â”€â”€ manager.py         # Gestionnaire des scrapers
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ listing_service.py # Service de gestion des annonces
â”œâ”€â”€ logs/                      # Fichiers de logs
â”œâ”€â”€ docker-compose.yml         # Configuration Docker Compose
â”œâ”€â”€ Dockerfile                 # Image Docker de l'application
â”œâ”€â”€ requirements.txt           # DÃ©pendances Python
â”œâ”€â”€ .env.example              # Exemple de configuration
â””â”€â”€ main.py                   # Point d'entrÃ©e principal
```

## âš™ï¸ Configuration

Les paramÃ¨tres sont configurÃ©s via le fichier `.env` :

### Base de donnÃ©es
- `POSTGRES_HOST`, `POSTGRES_PORT`, `POSTGRES_DB`, `POSTGRES_USER`, `POSTGRES_PASSWORD`

### Redis
- `REDIS_HOST`, `REDIS_PORT`

### Scheduler
- `SCHEDULER_ENABLED` - Activer/dÃ©sactiver le scheduler (true/false)
- `LEBONCOIN_INTERVAL_MINUTES` - Intervalle de scraping pour Leboncoin (en minutes)
- `SELOGER_INTERVAL_MINUTES` - Intervalle de scraping pour SeLoger
- `CARTE_COLOC_INTERVAL_MINUTES` - Intervalle de scraping pour La Carte des Coloc

### Scraping
- `USER_AGENT` - User-Agent pour les requÃªtes HTTP
- `REQUEST_TIMEOUT` - Timeout des requÃªtes (en secondes)
- `RETRY_ATTEMPTS` - Nombre de tentatives en cas d'Ã©chec

## ğŸ” Utilisation

### Scraping manuel

Pour exÃ©cuter un scraping une seule fois sans scheduler :

```bash
docker-compose exec app python -c "from app.scheduler import ScrapingScheduler; s = ScrapingScheduler(); s.run_once()"
```

Pour un site spÃ©cifique :
```bash
docker-compose exec app python -c "from app.scheduler import ScrapingScheduler; s = ScrapingScheduler(); s.run_once('leboncoin')"
```

### AccÃ©der Ã  la base de donnÃ©es

```bash
docker-compose exec postgres psql -U webown -d webown
```

### RequÃªtes SQL utiles

```sql
-- Voir toutes les annonces
SELECT * FROM listings ORDER BY last_updated DESC LIMIT 10;

-- Compter les annonces par source
SELECT source, COUNT(*) FROM listings GROUP BY source;

-- Voir les annonces actives
SELECT * FROM listings WHERE is_active = true;

-- Voir les annonces d'une ville spÃ©cifique
SELECT * FROM listings WHERE city ILIKE '%Paris%';
```

## ğŸ› ï¸ DÃ©veloppement

### Ajouter un nouveau scraper

1. CrÃ©er un nouveau fichier dans `app/scrapers/` (ex: `nouveau_site.py`)
2. HÃ©riter de `BaseScraper` et implÃ©menter les mÃ©thodes requises
3. Ajouter le scraper dans `ScraperManager` (`app/scrapers/manager.py`)
4. Ajouter un job dans le scheduler (`app/scheduler.py`)

Exemple :
```python
from app.scrapers.base import BaseScraper

class NouveauSiteScraper(BaseScraper):
    def __init__(self):
        super().__init__("nouveau_site")
    
    def scrape_listings(self, search_params=None):
        # ImplÃ©menter le scraping
        pass
    
    def parse_listing(self, listing_data):
        # ImplÃ©menter le parsing
        pass
```

### Tests

Les tests peuvent Ãªtre ajoutÃ©s dans un dossier `tests/` (Ã  crÃ©er).

## ğŸ“ Notes importantes

- Les sÃ©lecteurs CSS dans les scrapers peuvent nÃ©cessiter des ajustements si les sites web changent leur structure HTML
- Respectez les conditions d'utilisation des sites web scrapÃ©s
- Utilisez des intervalles raisonnables pour Ã©viter de surcharger les serveurs
- Les logs sont sauvegardÃ©s dans `logs/` avec rotation quotidienne

## ğŸ”’ SÃ©curitÃ©

- Ne commitez jamais le fichier `.env` avec des mots de passe rÃ©els
- Utilisez des mots de passe forts en production
- Configurez un firewall si nÃ©cessaire

## ğŸ“„ Licence

[Ã€ dÃ©finir]

## ğŸ¤ Contribution

[Ã€ dÃ©finir]

